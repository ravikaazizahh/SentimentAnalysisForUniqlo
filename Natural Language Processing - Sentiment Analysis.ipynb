{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3a5f94",
   "metadata": {},
   "source": [
    "## Busines Understanding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02ce69aa",
   "metadata": {},
   "source": [
    "The product I will discuss is Uniqlo. I chose the product because I wanted to analyze whether it was talked about positively or negatively in a public manner."
   ]
  },
  {
   "cell_type": "raw",
   "id": "db3bcede",
   "metadata": {},
   "source": [
    "Next, we do data collection, namely by importing or entering data into Python using pandas. The data we use is scraped from Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08467adf",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f41b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548ce89",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f752bb",
   "metadata": {},
   "source": [
    "#### Case Folding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38363a6f",
   "metadata": {},
   "source": [
    "I created a new column for this case folding result, Because if there is an error in processing data, it will make it easier for us to reprocess data because the original tweet has not changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5313a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "df['clean_tweet'] = df['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f68cc23",
   "metadata": {},
   "source": [
    "Then next we will remove special characters like '\"!@#$%^&*() and will also remove numbers attached to a word using the regex module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfad1a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GA 2 tumbler kece by Uniqlo new untuk 2 orang ...</td>\n",
       "      <td>ga  tumbler kece by uniqlo new untuk  orang si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "      <td>udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sering padahal begini Uniqlo pernah dapet UT g...</td>\n",
       "      <td>sering padahal begini uniqlo pernah dapet ut g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...</td>\n",
       "      <td>masih ada ga klu ada besok gw samperin uniqlo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wiiii uniqlo lagi diskon yuk</td>\n",
       "      <td>wiiii uniqlo lagi diskon yuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>Jaman SMA pengen ke mall Kuningan apa tu jauh ...</td>\n",
       "      <td>jaman sma pengen ke mall kuningan apa tu jauh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>Saya sih pengguna Uniqlo H amp M dan Levi s se...</td>\n",
       "      <td>saya sih pengguna uniqlo h amp m dan levi s se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>Uniqlo kapan diskon lg si</td>\n",
       "      <td>uniqlo kapan diskon lg si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>Uniqlo</td>\n",
       "      <td>uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>1666</td>\n",
       "      <td>halo Uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "      <td>halo uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  GA 2 tumbler kece by Uniqlo new untuk 2 orang ...   \n",
       "1              1     Udah gajian tetep ga sanggup belanja di uniqlo   \n",
       "2              2  Sering padahal begini Uniqlo pernah dapet UT g...   \n",
       "3              3  MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...   \n",
       "4              4                       Wiiii uniqlo lagi diskon yuk   \n",
       "...          ...                                                ...   \n",
       "1662        1662  Jaman SMA pengen ke mall Kuningan apa tu jauh ...   \n",
       "1663        1663  Saya sih pengguna Uniqlo H amp M dan Levi s se...   \n",
       "1664        1664                          Uniqlo kapan diskon lg si   \n",
       "1665        1665                                             Uniqlo   \n",
       "1666        1666  halo Uniqlo apakah sedang mencari tim kreatif ...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     ga  tumbler kece by uniqlo new untuk  orang si...  \n",
       "1        udah gajian tetep ga sanggup belanja di uniqlo  \n",
       "2     sering padahal begini uniqlo pernah dapet ut g...  \n",
       "3     masih ada ga klu ada besok gw samperin uniqlo ...  \n",
       "4                          wiiii uniqlo lagi diskon yuk  \n",
       "...                                                 ...  \n",
       "1662  jaman sma pengen ke mall kuningan apa tu jauh ...  \n",
       "1663  saya sih pengguna uniqlo h amp m dan levi s se...  \n",
       "1664                          uniqlo kapan diskon lg si  \n",
       "1665                                             uniqlo  \n",
       "1666  halo uniqlo apakah sedang mencari tim kreatif ...  \n",
       "\n",
       "[1667 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#to delete a number attached to a word in the 'clean_tweet' column of the df DataFrame\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "#to remove special characters other than letters, numbers, and spaces in the 'clean_tweet' column of the df DataFrame.\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "\n",
    "#to remove punctuation inside column 'clean_tweet' on DataFrame df.\n",
    "df['clean_tweet'] = df['clean_tweet'].str.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ded1e",
   "metadata": {},
   "source": [
    "#### Clear Emoji"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e61dfa81",
   "metadata": {},
   "source": [
    "Furthermore, because this tweet data is obtained from Twitter directly, usually people use emojis in their tweets, so we will delete the emoji."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddc8fa74",
   "metadata": {},
   "source": [
    "before we create a function to delete emojis, \n",
    "we will remove the spaces at the beginning and end of the text and also \n",
    "separates each word in the text into separate elements in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89405917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GA 2 tumbler kece by Uniqlo new untuk 2 orang ...</td>\n",
       "      <td>ga tumbler kece by uniqlo new untuk orang sila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "      <td>udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sering padahal begini Uniqlo pernah dapet UT g...</td>\n",
       "      <td>sering padahal begini uniqlo pernah dapet ut g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...</td>\n",
       "      <td>masih ada ga klu ada besok gw samperin uniqlo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wiiii uniqlo lagi diskon yuk</td>\n",
       "      <td>wiiii uniqlo lagi diskon yuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>Jaman SMA pengen ke mall Kuningan apa tu jauh ...</td>\n",
       "      <td>jaman sma pengen ke mall kuningan apa tu jauh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>Saya sih pengguna Uniqlo H amp M dan Levi s se...</td>\n",
       "      <td>saya sih pengguna uniqlo h amp m dan levi s se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>Uniqlo kapan diskon lg si</td>\n",
       "      <td>uniqlo kapan diskon lg si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>Uniqlo</td>\n",
       "      <td>uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>1666</td>\n",
       "      <td>halo Uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "      <td>halo uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  GA 2 tumbler kece by Uniqlo new untuk 2 orang ...   \n",
       "1              1     Udah gajian tetep ga sanggup belanja di uniqlo   \n",
       "2              2  Sering padahal begini Uniqlo pernah dapet UT g...   \n",
       "3              3  MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...   \n",
       "4              4                       Wiiii uniqlo lagi diskon yuk   \n",
       "...          ...                                                ...   \n",
       "1662        1662  Jaman SMA pengen ke mall Kuningan apa tu jauh ...   \n",
       "1663        1663  Saya sih pengguna Uniqlo H amp M dan Levi s se...   \n",
       "1664        1664                          Uniqlo kapan diskon lg si   \n",
       "1665        1665                                             Uniqlo   \n",
       "1666        1666  halo Uniqlo apakah sedang mencari tim kreatif ...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     ga tumbler kece by uniqlo new untuk orang sila...  \n",
       "1        udah gajian tetep ga sanggup belanja di uniqlo  \n",
       "2     sering padahal begini uniqlo pernah dapet ut g...  \n",
       "3     masih ada ga klu ada besok gw samperin uniqlo ...  \n",
       "4                          wiiii uniqlo lagi diskon yuk  \n",
       "...                                                 ...  \n",
       "1662  jaman sma pengen ke mall kuningan apa tu jauh ...  \n",
       "1663  saya sih pengguna uniqlo h amp m dan levi s se...  \n",
       "1664                          uniqlo kapan diskon lg si  \n",
       "1665                                             uniqlo  \n",
       "1666  halo uniqlo apakah sedang mencari tim kreatif ...  \n",
       "\n",
       "[1667 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to remove spaces at the beginning and end of the text, we use .strip in the string method\n",
    "df['clean_tweet'] = df['clean_tweet'].str.strip()\n",
    "\n",
    "#to separate each word in the text into separate elements in a list, we use the .split in the string method\n",
    "df['clean_tweet'] = df['clean_tweet'].str.split()\n",
    "\n",
    "def emoji_cleaner(data):\n",
    "    word_emoji = data\n",
    "    word_list = [] #a variable that contains a list to store the initial text and an empty list that will contain the words after the emoji is removed.\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                    \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    for word in word_emoji:\n",
    "        emoji_pattern.sub(r'', word) \n",
    "        word_list.append(word)\n",
    "    \n",
    "    data = ' '.join(word_list)\n",
    "    return data\n",
    "\n",
    "#to implement the function, we can use the .apply method\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: emoji_cleaner(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228b378",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb169ebb",
   "metadata": {},
   "source": [
    "After doing case folding, we will go to the next process in the pre-processing process, namely tokenizing. This tokenizing process uses NLTK modules and word_tokenize functions. This function is used to divide text into tokens, which are words or other units that are meaningful in the context of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b711d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizing(data):\n",
    "    data = word_tokenize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b431d",
   "metadata": {},
   "source": [
    "#### Turning Slang Words Into Standard Words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a8c0afc",
   "metadata": {},
   "source": [
    "Next we'll modify it slightly to change it to default and/or delete the slang word in the tweet."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d0d63a8",
   "metadata": {},
   "source": [
    "Still in the series of pre-processing processes, then we enter filtering, which is in it there are stopwords and lemmatization. Here we still use the NLTK module, the difference is that only the function called now has stopwords and word net lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2664eaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GA 2 tumbler kece by Uniqlo new untuk 2 orang ...</td>\n",
       "      <td>tidak tumbler kece by uniqlo new untuk orang s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "      <td>udah gajian tetep tidak sanggup belanja di uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sering padahal begini Uniqlo pernah dapet UT g...</td>\n",
       "      <td>sering padahal begini uniqlo pernah dapet ut g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...</td>\n",
       "      <td>masih ada tidak klu ada besok gw samperin uniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wiiii uniqlo lagi diskon yuk</td>\n",
       "      <td>wiiii uniqlo lagi diskon yuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>Jaman SMA pengen ke mall Kuningan apa tu jauh ...</td>\n",
       "      <td>jaman sma pengen ke mall kuningan apa tu jauh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>Saya sih pengguna Uniqlo H amp M dan Levi s se...</td>\n",
       "      <td>saya sih pengguna uniqlo h amp m dan levi s se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>Uniqlo kapan diskon lg si</td>\n",
       "      <td>uniqlo kapan diskon lg si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>Uniqlo</td>\n",
       "      <td>uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>1666</td>\n",
       "      <td>halo Uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "      <td>halo uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  GA 2 tumbler kece by Uniqlo new untuk 2 orang ...   \n",
       "1              1     Udah gajian tetep ga sanggup belanja di uniqlo   \n",
       "2              2  Sering padahal begini Uniqlo pernah dapet UT g...   \n",
       "3              3  MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...   \n",
       "4              4                       Wiiii uniqlo lagi diskon yuk   \n",
       "...          ...                                                ...   \n",
       "1662        1662  Jaman SMA pengen ke mall Kuningan apa tu jauh ...   \n",
       "1663        1663  Saya sih pengguna Uniqlo H amp M dan Levi s se...   \n",
       "1664        1664                          Uniqlo kapan diskon lg si   \n",
       "1665        1665                                             Uniqlo   \n",
       "1666        1666  halo Uniqlo apakah sedang mencari tim kreatif ...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     tidak tumbler kece by uniqlo new untuk orang s...  \n",
       "1     udah gajian tetep tidak sanggup belanja di uniqlo  \n",
       "2     sering padahal begini uniqlo pernah dapet ut g...  \n",
       "3     masih ada tidak klu ada besok gw samperin uniq...  \n",
       "4                          wiiii uniqlo lagi diskon yuk  \n",
       "...                                                 ...  \n",
       "1662  jaman sma pengen ke mall kuningan apa tu jauh ...  \n",
       "1663  saya sih pengguna uniqlo h amp m dan levi s se...  \n",
       "1664                          uniqlo kapan diskon lg si  \n",
       "1665                                             uniqlo  \n",
       "1666  halo uniqlo apakah sedang mencari tim kreatif ...  \n",
       "\n",
       "[1667 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we create a list to later fill in the slang words2 and their replacements. \n",
    "slang_list = [('yg','yang'),\n",
    "              ('ga','tidak'),\n",
    "              ('gak','tidak'),\n",
    "              ('ya','iya'),\n",
    "              ('aja','saja'),\n",
    "              ('kalo','kalau'),\n",
    "              ('gue','saya'),\n",
    "              ('aku','saya'),\n",
    "              ('bgt','banget')\n",
    "             ]\n",
    "\n",
    "# Create a function, with data parameters and slang_list\n",
    "def slang_cleaner(data, slang_list:list=[]):\n",
    "    word_token = tokenizing(data)\n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    for word in word_token:\n",
    "        for slang in slang_list:\n",
    "            if word.lower() in slang[0].lower():\n",
    "                word = slang[1]\n",
    "                break\n",
    "        word_list.append(word)\n",
    "        \n",
    "    data = ' '.join(word_list)\n",
    "    return data\n",
    "#To implement the function, we can use the .apply method\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: slang_cleaner(x, slang_list))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc55aa",
   "metadata": {},
   "source": [
    "##  Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c166501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GA 2 tumbler kece by Uniqlo new untuk 2 orang ...</td>\n",
       "      <td>tumbler kece by uniqlo new orang silakan reply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "      <td>udah gajian tetep sanggup belanja uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sering padahal begini Uniqlo pernah dapet UT g...</td>\n",
       "      <td>uniqlo dapet ut graphic harga doang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...</td>\n",
       "      <td>klu besok gw samperin uniqlo deket rumah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wiiii uniqlo lagi diskon yuk</td>\n",
       "      <td>wiiii uniqlo diskon yuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>Jaman SMA pengen ke mall Kuningan apa tu jauh ...</td>\n",
       "      <td>jaman sma pengen mall kuningan tu pengen liat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>Saya sih pengguna Uniqlo H amp M dan Levi s se...</td>\n",
       "      <td>sih pengguna uniqlo h amp m levi s sejati hasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>Uniqlo kapan diskon lg si</td>\n",
       "      <td>uniqlo diskon lg si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>Uniqlo</td>\n",
       "      <td>uniqlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>1666</td>\n",
       "      <td>halo Uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "      <td>halo uniqlo mencari tim kreatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  GA 2 tumbler kece by Uniqlo new untuk 2 orang ...   \n",
       "1              1     Udah gajian tetep ga sanggup belanja di uniqlo   \n",
       "2              2  Sering padahal begini Uniqlo pernah dapet UT g...   \n",
       "3              3  MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...   \n",
       "4              4                       Wiiii uniqlo lagi diskon yuk   \n",
       "...          ...                                                ...   \n",
       "1662        1662  Jaman SMA pengen ke mall Kuningan apa tu jauh ...   \n",
       "1663        1663  Saya sih pengguna Uniqlo H amp M dan Levi s se...   \n",
       "1664        1664                          Uniqlo kapan diskon lg si   \n",
       "1665        1665                                             Uniqlo   \n",
       "1666        1666  halo Uniqlo apakah sedang mencari tim kreatif ...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     tumbler kece by uniqlo new orang silakan reply...  \n",
       "1              udah gajian tetep sanggup belanja uniqlo  \n",
       "2                   uniqlo dapet ut graphic harga doang  \n",
       "3              klu besok gw samperin uniqlo deket rumah  \n",
       "4                               wiiii uniqlo diskon yuk  \n",
       "...                                                 ...  \n",
       "1662  jaman sma pengen mall kuningan tu pengen liat ...  \n",
       "1663  sih pengguna uniqlo h amp m levi s sejati hasi...  \n",
       "1664                                uniqlo diskon lg si  \n",
       "1665                                             uniqlo  \n",
       "1666                    halo uniqlo mencari tim kreatif  \n",
       "\n",
       "[1667 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# first we use .strip to remove spaces that are at the beginning and end of the text\n",
    "df['clean_tweet'] = df['clean_tweet'].str.strip()\n",
    "\n",
    "# then we use .split to Split each word in the text into separate elements in a list\n",
    "df['clean_tweet'] = df['clean_tweet'].str.split()\n",
    "\n",
    "# Create a function that has parameters word_list\n",
    "def clean_stopwords(word_list):\n",
    "    \n",
    "    # Create an empty list processed_word_list that will be used to store processed words.\n",
    "    processed_word_list = []\n",
    "    for word in word_list: # Through the for loop, every word in the list word_list checked. \n",
    "        word = word.lower() # Change words to lowercase to ensure consistency.\n",
    "        \n",
    "        # Create an 'if' to check if the word is not a stopwords in Indonesian. \n",
    "        # If it is not a stopword, then the word is considered important and will be processed further.\n",
    "        if word not in stopwords.words(\"Indonesian\"): \n",
    "            # then lemmatize the word using the lemmatizer that was defined earlier. \n",
    "            # In this case, lemmatize to restore the word in its basic form.\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            \n",
    "            # then add the processed word into the processed_word_list variable.\n",
    "            processed_word_list.append(word)\n",
    "    return processed_word_list\n",
    "\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(clean_stopwords)\n",
    "df['clean_tweet'] = df['clean_tweet'].str.join(' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1be87",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0798c135",
   "metadata": {},
   "source": [
    "Next we go into the data processing process, where here we will count the number of positive or negative words that will later give a score in the tweet. Previously we will import the numpy module and read the positive and negative word files that we downloaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e991ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanggup\n",
      " ['positif']\n",
      "resmi\n",
      " ['positif']\n",
      "resmi\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "dibantu\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "bangga\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "bosan\n",
      " ['negative']\n",
      "bosan\n",
      " ['negative']\n",
      "miring\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "salah\n",
      " ['negative']\n",
      "tutup\n",
      " ['negative']\n",
      "marah\n",
      " ['negative']\n",
      "marah\n",
      " ['negative']\n",
      "sanggup\n",
      " ['positif']\n",
      "awas\n",
      " ['negative']\n",
      "galau\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "lupa\n",
      " ['negative']\n",
      "salah\n",
      " ['negative']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "suka\n",
      " ['positif']\n",
      "happy\n",
      " ['positif']\n",
      "semoga\n",
      " ['positif']\n",
      "sehat\n",
      " ['positif']\n",
      "sehat\n",
      " ['positif']\n",
      "senang\n",
      " ['positif']\n",
      "senang\n",
      " ['positif']\n",
      "memperhatikan\n",
      " ['positif']\n",
      "disandera\n",
      " ['negative']\n",
      "meratapi\n",
      " ['negative']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "ayo\n",
      " ['positif']\n",
      "mentok\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "keren\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "macet\n",
      " ['negative']\n",
      "menolak\n",
      " ['negative']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "menghadiri\n",
      " ['positif']\n",
      "berhasil\n",
      " ['positif']\n",
      "lolos\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "sedih\n",
      " ['negative']\n",
      "keren\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "gampang\n",
      " ['positif']\n",
      "judi\n",
      " ['negative']\n",
      "happy\n",
      " ['positif']\n",
      "kelupaan\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "miskin\n",
      " ['negative']\n",
      "miskin\n",
      " ['negative']\n",
      "selamat\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "lembut\n",
      " ['positif']\n",
      "gerah\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "tragedi\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "percaya\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "menghilang\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "cepat\n",
      " ['positif']\n",
      "resmi\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "meracuni\n",
      " ['negative']\n",
      "pergi\n",
      " ['negative']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "ganggu\n",
      " ['negative']\n",
      "korban\n",
      " ['negative']\n",
      "setuju\n",
      " ['positif']\n",
      "setuju\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "pendek\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "kalah\n",
      " ['negative']\n",
      "spesial\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "kalap\n",
      " ['negative']\n",
      "heran\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "ok\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "sayang\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "lupa\n",
      " ['negative']\n",
      "enteng\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "bohong\n",
      " ['negative']\n",
      "bohong\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "miskin\n",
      " ['negative']\n",
      "miskin\n",
      " ['negative']\n",
      "ayo\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "takut\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "kasar\n",
      " ['negative']\n",
      "gampang\n",
      " ['positif']\n",
      "kasih\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n",
      "beruntung\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "mentok\n",
      " ['negative']\n",
      "kalah\n",
      " ['negative']\n",
      "kalap\n",
      " ['negative']\n",
      "cinta\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "kalap\n",
      " ['negative']\n",
      "puas\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "ok\n",
      " ['positif']\n",
      "jago\n",
      " ['positif']\n",
      "jago\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "kalap\n",
      " ['negative']\n",
      "legal\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "kasih\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "sakit\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "resmi\n",
      " ['positif']\n",
      "resmi\n",
      " ['positif']\n",
      "pamer\n",
      " ['negative']\n",
      "andalan\n",
      " ['positif']\n",
      "panik\n",
      " ['negative']\n",
      "macet\n",
      " ['negative']\n",
      "lelah\n",
      " ['negative']\n",
      "ok\n",
      " ['positif']\n",
      "mundur\n",
      " ['negative']\n",
      "lembut\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n",
      "urung\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "kasih\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "menang\n",
      " ['positif']\n",
      "anjing\n",
      " ['negative']\n",
      "aneh\n",
      " ['negative']\n",
      "indah\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "parah\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "kasih\n",
      " ['positif']\n",
      "hadiah\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "bangga\n",
      " ['positif']\n",
      "sindir\n",
      " ['negative']\n",
      "estetik\n",
      " ['positif']\n",
      "sakit\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "ibadah\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "lusuh\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "makan\n",
      " ['positif']\n",
      "sesuai\n",
      " ['positif']\n",
      "pendek\n",
      " ['negative']\n",
      "pendek\n",
      " ['negative']\n",
      "kehabisan\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "beruntung\n",
      " ['positif']\n",
      "minder\n",
      " ['negative']\n",
      "hadiah\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "andalan\n",
      " ['positif']\n",
      "pergi\n",
      " ['negative']\n",
      "aman\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "bonus\n",
      " ['positif']\n",
      "dadakan\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "kolot\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "susah\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n",
      "sesuai\n",
      " ['positif']\n",
      "anjing\n",
      " ['negative']\n",
      "legendaris\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "risau\n",
      " ['negative']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "unik\n",
      " ['positif']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "legendaris\n",
      " ['positif']\n",
      "unik\n",
      " ['positif']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "turun\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "cakap\n",
      " ['positif']\n",
      "kalah\n",
      " ['negative']\n",
      "lupa\n",
      " ['negative']\n",
      "hadiah\n",
      " ['positif']\n",
      "hadiah\n",
      " ['positif']\n",
      "sayang\n",
      " ['positif']\n",
      "awas\n",
      " ['negative']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "tutup\n",
      " ['negative']\n",
      "keren\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "makan\n",
      " ['positif']\n",
      "hadiah\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "fakta\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "ramah\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "ayo\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "lupa\n",
      " ['negative']\n",
      "alhamdulillah\n",
      " ['positif']\n",
      "happy\n",
      " ['positif']\n",
      "maju\n",
      " ['positif']\n",
      "mundur\n",
      " ['negative']\n",
      "ketiduran\n",
      " ['negative']\n",
      "ketiduran\n",
      " ['negative']\n",
      "pergi\n",
      " ['negative']\n",
      "belajar\n",
      " ['positif']\n",
      "elok\n",
      " ['positif']\n",
      "pusing\n",
      " ['negative']\n",
      "pusing\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "semoga\n",
      " ['positif']\n",
      "isu\n",
      " ['negative']\n",
      "sobat\n",
      " ['positif']\n",
      "oknum\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "sesuai\n",
      " ['positif']\n",
      "lembut\n",
      " ['positif']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "rusak\n",
      " ['negative']\n",
      "tutup\n",
      " ['negative']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "terbaik\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "bonus\n",
      " ['positif']\n",
      "dadakan\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "hadiah\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "pergi\n",
      " ['negative']\n",
      "cocok\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "payah\n",
      " ['negative']\n",
      "sedih\n",
      " ['negative']\n",
      "senang\n",
      " ['positif']\n",
      "senang\n",
      " ['positif']\n",
      "repot\n",
      " ['negative']\n",
      "ayo\n",
      " ['positif']\n",
      "ayo\n",
      " ['positif']\n",
      "pendek\n",
      " ['negative']\n",
      "cocok\n",
      " ['positif']\n",
      "menang\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "izin\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "pendek\n",
      " ['negative']\n",
      "cakap\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "bangga\n",
      " ['positif']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "suka\n",
      " ['positif']\n",
      "takut\n",
      " ['negative']\n",
      "salah\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "kaku\n",
      " ['negative']\n",
      "kasih\n",
      " ['positif']\n",
      "marah\n",
      " ['negative']\n",
      "marah\n",
      " ['negative']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "kreasi\n",
      " ['positif']\n",
      "mampus\n",
      " ['negative']\n",
      "parah\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enak\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "ketinggalan\n",
      " ['negative']\n",
      "setuju\n",
      " ['positif']\n",
      "setuju\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "suka\n",
      " ['positif']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "konspirasi\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "mesum\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "capek\n",
      " ['negative']\n",
      "capek\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "pergi\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "ternama\n",
      " ['positif']\n",
      "menghentikan\n",
      " ['positif']\n",
      "pergi\n",
      " ['negative']\n",
      "canggih\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "wajar\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "makan\n",
      " ['positif']\n",
      "selesai\n",
      " ['positif']\n",
      "pergi\n",
      " ['negative']\n",
      "sesuai\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "mantap\n",
      " ['positif']\n",
      "mantap\n",
      " ['positif']\n",
      "ternama\n",
      " ['positif']\n",
      "menghentikan\n",
      " ['positif']\n",
      "hikmah\n",
      " ['positif']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "spesial\n",
      " ['positif']\n",
      "hadiah\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "emosi\n",
      " ['negative']\n",
      "spesial\n",
      " ['positif']\n",
      "mudah\n",
      " ['positif']\n",
      "kokoh\n",
      " ['positif']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "suka\n",
      " ['positif']\n",
      "gerah\n",
      " ['negative']\n",
      "turun\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "pergi\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "pergi\n",
      " ['negative']\n",
      "gratis\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "tutup\n",
      " ['negative']\n",
      "aduh\n",
      " ['negative']\n",
      "lupa\n",
      " ['negative']\n",
      "gerah\n",
      " ['negative']\n",
      "makan\n",
      " ['positif']\n",
      "motivasi\n",
      " ['positif']\n",
      "rajin\n",
      " ['positif']\n",
      "khawatir\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "pertumbuhan\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "mantap\n",
      " ['positif']\n",
      "mantap\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "afdol\n",
      " ['positif']\n",
      "pendek\n",
      " ['negative']\n",
      "dimaki\n",
      " ['negative']\n",
      "jahat\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "sesuai\n",
      " ['positif']\n",
      "selamat\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "malas\n",
      " ['negative']\n",
      "malas\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "dadakan\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "doakan\n",
      " ['positif']\n",
      "semoga\n",
      " ['positif']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "ramah\n",
      " ['positif']\n",
      "memuaskan\n",
      " ['positif']\n",
      "tegar\n",
      " ['positif']\n",
      "impulsif\n",
      " ['negative']\n",
      "iri\n",
      " ['negative']\n",
      "iri\n",
      " ['negative']\n",
      "ogah\n",
      " ['negative']\n",
      "susah\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "mulus\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "tai\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "maju\n",
      " ['positif']\n",
      "mundur\n",
      " ['negative']\n",
      "menarik\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "jorok\n",
      " ['negative']\n",
      "sampah\n",
      " ['negative']\n",
      "sampah\n",
      " ['negative']\n",
      "sampah\n",
      " ['negative']\n",
      "berulat\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "suka\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "keren\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "aduh\n",
      " ['negative']\n",
      "rugi\n",
      " ['negative']\n",
      "rugi\n",
      " ['negative']\n",
      "rugi\n",
      " ['negative']\n",
      "rugi\n",
      " ['negative']\n",
      "senang\n",
      " ['positif']\n",
      "senang\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "murahan\n",
      " ['negative']\n",
      "sayang\n",
      " ['positif']\n",
      "colong\n",
      " ['negative']\n",
      "suka\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "sayang\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "dilarang\n",
      " ['negative']\n",
      "lusuh\n",
      " ['negative']\n",
      "sesuai\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "pergi\n",
      " ['negative']\n",
      "pergi\n",
      " ['negative']\n",
      "final\n",
      " ['positif']\n",
      "cepat\n",
      " ['positif']\n",
      "heran\n",
      " ['negative']\n",
      "lusuh\n",
      " ['negative']\n",
      "hadiah\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n",
      "terbaik\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "aktif\n",
      " ['positif']\n",
      "sadar\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "gerah\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "puas\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "lupa\n",
      " ['negative']\n",
      "cakap\n",
      " ['positif']\n",
      "gerah\n",
      " ['negative']\n",
      "kasar\n",
      " ['negative']\n",
      "menarik\n",
      " ['positif']\n",
      "legendaris\n",
      " ['positif']\n",
      "sesuai\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "unik\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "kolaborasi\n",
      " ['positif']\n",
      "spesial\n",
      " ['positif']\n",
      "elok\n",
      " ['positif']\n",
      "ok\n",
      " ['positif']\n",
      "cepat\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "puas\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "cocok\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "indah\n",
      " ['positif']\n",
      "semangat\n",
      " ['positif']\n",
      "berkreasi\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "alhamdulillah\n",
      " ['positif']\n",
      "dukungan\n",
      " ['positif']\n",
      "berhasil\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "terpilih\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "berhasil\n",
      " ['positif']\n",
      "polusi\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "sabar\n",
      " ['positif']\n",
      "sabar\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "pendek\n",
      " ['negative']\n",
      "luka\n",
      " ['negative']\n",
      "komitmen\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "aneh\n",
      " ['negative']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "emang\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "jelek\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "sabar\n",
      " ['positif']\n",
      "sabar\n",
      " ['positif']\n",
      "jelek\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n",
      "murah\n",
      " ['positif']\n",
      "miskin\n",
      " ['negative']\n",
      "miskin\n",
      " ['negative']\n",
      "sabar\n",
      " ['positif']\n",
      "sabar\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "spesial\n",
      " ['positif']\n",
      "favorit\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "ok\n",
      " ['positif']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "rezeki\n",
      " ['positif']\n",
      "rezeki\n",
      " ['positif']\n",
      "lupa\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "jelek\n",
      " ['negative']\n",
      "ribet\n",
      " ['negative']\n",
      "ribet\n",
      " ['negative']\n",
      "kerja\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "izin\n",
      " ['positif']\n",
      "racun\n",
      " ['negative']\n",
      "keracunan\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "miring\n",
      " ['negative']\n",
      "jujur\n",
      " ['positif']\n",
      "jujur\n",
      " ['positif']\n",
      "gerah\n",
      " ['negative']\n",
      "favorit\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "terkenal\n",
      " ['positif']\n",
      "gap\n",
      " ['negative']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "anjing\n",
      " ['negative']\n",
      "spesial\n",
      " ['positif']\n",
      "aman\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "sadar\n",
      " ['positif']\n",
      "sopan\n",
      " ['positif']\n",
      "rapi\n",
      " ['positif']\n",
      "taat\n",
      " ['positif']\n",
      "mandiri\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "keren\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "ayo\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "aman\n",
      " ['positif']\n",
      "pergi\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "minus\n",
      " ['negative']\n",
      "kasih\n",
      " ['positif']\n",
      "hadiah\n",
      " ['positif']\n",
      "alhamdulillah\n",
      " ['positif']\n",
      "bagus\n",
      " ['positif']\n",
      "solusi\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "rapih\n",
      " ['positif']\n",
      "sampah\n",
      " ['negative']\n",
      "racun\n",
      " ['negative']\n",
      "enak\n",
      " ['positif']\n",
      "terbaik\n",
      " ['positif']\n",
      "lupa\n",
      " ['negative']\n",
      "menjaga\n",
      " ['positif']\n",
      "kesopanan\n",
      " ['positif']\n",
      "anjing\n",
      " ['negative']\n",
      "kehabisan\n",
      " ['negative']\n",
      "ayo\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "maaf\n",
      " ['positif']\n",
      "miris\n",
      " ['negative']\n",
      "gratis\n",
      " ['positif']\n",
      "capek\n",
      " ['negative']\n",
      "capek\n",
      " ['negative']\n",
      "makan\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "turun\n",
      " ['negative']\n",
      "bohong\n",
      " ['negative']\n",
      "bohong\n",
      " ['negative']\n",
      "lembut\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "tai\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "wajar\n",
      " ['positif']\n",
      "hidup\n",
      " ['positif']\n",
      "ibadah\n",
      " ['positif']\n",
      "pamer\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "berusaha\n",
      " ['positif']\n",
      "bonus\n",
      " ['positif']\n",
      "hidup\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "gila\n",
      " ['negative']\n",
      "marah\n",
      " ['negative']\n",
      "marah\n",
      " ['negative']\n",
      "hidup\n",
      " ['positif']\n",
      "cocok\n",
      " ['positif']\n",
      "ketinggalan\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "takut\n",
      " ['negative']\n",
      "jorok\n",
      " ['negative']\n",
      "lucu\n",
      " ['positif']\n",
      "lucu\n",
      " ['positif']\n",
      "setuju\n",
      " ['positif']\n",
      "setuju\n",
      " ['positif']\n",
      "solid\n",
      " ['positif']\n",
      "pendek\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "salah\n",
      " ['negative']\n",
      "happy\n",
      " ['positif']\n",
      "cepat\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "apresiasi\n",
      " ['positif']\n",
      "enak\n",
      " ['positif']\n",
      "menarik\n",
      " ['positif']\n",
      "bersahabat\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "nyaman\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "minus\n",
      " ['negative']\n",
      "minus\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "bingung\n",
      " ['negative']\n",
      "minus\n",
      " ['negative']\n",
      "minus\n",
      " ['negative']\n",
      "pergi\n",
      " ['negative']\n",
      "bagus\n",
      " ['positif']\n",
      "lawan\n",
      " ['negative']\n",
      "sayang\n",
      " ['positif']\n",
      "gratis\n",
      " ['positif']\n",
      "bonus\n",
      " ['positif']\n",
      "dadakan\n",
      " ['negative']\n",
      "murah\n",
      " ['positif']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emang\n",
      " ['negative']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "cakep\n",
      " ['positif']\n",
      "suka\n",
      " ['positif']\n",
      "mahal\n",
      " ['negative']\n",
      "mahal\n",
      " ['negative']\n",
      "lupa\n",
      " ['negative']\n",
      "gelap\n",
      " ['negative']\n",
      "salah\n",
      " ['negative']\n",
      "emang\n",
      " ['negative']\n",
      "lusuh\n",
      " ['negative']\n",
      "gratis\n",
      " ['positif']\n",
      "kerja\n",
      " ['positif']\n",
      "makan\n",
      " ['positif']\n",
      "makan\n",
      " ['positif']\n",
      "percaya\n",
      " ['positif']\n",
      "lolos\n",
      " ['positif']\n",
      "kreatif\n",
      " ['positif']\n",
      "Nilai rata-rata: 0.10317936412717456\n",
      "Standar deviasi: 0.8939754198029346\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pos_list= open(\"./kata_positif.txt\",\"r\")\n",
    "pos_kata = pos_list.readlines()\n",
    "neg_list= open(\"./kata_negatif.txt\",\"r\")\n",
    "neg_kata = neg_list.readlines()\n",
    "\n",
    "# Change a clean_tweet column to a list\n",
    "items = df['clean_tweet'].tolist()\n",
    "\n",
    "# Create an empty list of results that will be used to store the results of calculating positive and negative values from each text.\n",
    "hasil = []\n",
    "\n",
    "# Create a list that contains words that signify the antithesis or opposite of the next words in the text.\n",
    "list_anti = ['tidak','lawan','anti', 'belum', 'belom', 'tdk', 'jangan', 'gak', 'enggak', 'bukan', 'sulit', 'tak', 'sblm']\n",
    "\n",
    "# create loops to perform positive and negative value calculations.\n",
    "for item in items:\n",
    "    tweets = item.strip().split()\n",
    "    \n",
    "    count_p = 0 # Positive value\n",
    "    count_n = 0 # Negatif value\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        '''\n",
    "        create loops and conditions to Check positive words, \n",
    "        If there is a match, then check whether the previous word is included in the list_anti. \n",
    "        If so, then the word is considered negative, so count_n plus 1. \n",
    "        If not, then the word is considered positive, so count_p plus 1.\n",
    "        '''\n",
    "        for kata_pos in pos_kata:\n",
    "            if kata_pos.strip().lower() == tweet.lower():\n",
    "                if items[items.index(item)-1] in list_anti:\n",
    "                    print(items[items.index(item)-1], kata_pos, ['negatif'])\n",
    "                    count_n += 1\n",
    "                else:\n",
    "                    print(kata_pos, ['positif'])\n",
    "                    count_p += 1\n",
    "                    \n",
    "        '''\n",
    "        create loops and conditions to Check negative words, \n",
    "        If there is a match, then check whether the previous word is included in the list_anti. \n",
    "        If so, then the word is considered positive, so count_p plus 1. \n",
    "        If not, then the word is considered negative, so count_n plus 1.\n",
    "        '''\n",
    "        for kata_neg in neg_kata:\n",
    "            if kata_neg.strip().lower() == tweet.lower():\n",
    "                if items[items.index(item)-1] in list_anti:\n",
    "                    print(items[items.index(item)-1], kata_neg, ['positive'])\n",
    "                    count_p += 1\n",
    "                else:\n",
    "                    print(kata_neg, ['negative'])\n",
    "                    count_n += 1\n",
    "    # After all the words in the text are processed, calculate the score results obtained\n",
    "    hasil.append(count_p - count_n)\n",
    "\n",
    "# Finally, show the average value and standard deviation:\n",
    "print (\"Nilai rata-rata: \"+str(np.mean(hasil)))\n",
    "print (\"Standar deviasi: \"+str(np.std(hasil)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb269b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GA 2 tumbler kece by Uniqlo new untuk 2 orang ...</td>\n",
       "      <td>tumbler kece by uniqlo new orang silakan reply...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "      <td>udah gajian tetep sanggup belanja uniqlo</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sering padahal begini Uniqlo pernah dapet UT g...</td>\n",
       "      <td>uniqlo dapet ut graphic harga doang</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...</td>\n",
       "      <td>klu besok gw samperin uniqlo deket rumah</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wiiii uniqlo lagi diskon yuk</td>\n",
       "      <td>wiiii uniqlo diskon yuk</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>Jaman SMA pengen ke mall Kuningan apa tu jauh ...</td>\n",
       "      <td>jaman sma pengen mall kuningan tu pengen liat ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>Saya sih pengguna Uniqlo H amp M dan Levi s se...</td>\n",
       "      <td>sih pengguna uniqlo h amp m levi s sejati hasi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>Uniqlo kapan diskon lg si</td>\n",
       "      <td>uniqlo diskon lg si</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>Uniqlo</td>\n",
       "      <td>uniqlo</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>1666</td>\n",
       "      <td>halo Uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "      <td>halo uniqlo mencari tim kreatif</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  GA 2 tumbler kece by Uniqlo new untuk 2 orang ...   \n",
       "1              1     Udah gajian tetep ga sanggup belanja di uniqlo   \n",
       "2              2  Sering padahal begini Uniqlo pernah dapet UT g...   \n",
       "3              3  MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...   \n",
       "4              4                       Wiiii uniqlo lagi diskon yuk   \n",
       "...          ...                                                ...   \n",
       "1662        1662  Jaman SMA pengen ke mall Kuningan apa tu jauh ...   \n",
       "1663        1663  Saya sih pengguna Uniqlo H amp M dan Levi s se...   \n",
       "1664        1664                          Uniqlo kapan diskon lg si   \n",
       "1665        1665                                             Uniqlo   \n",
       "1666        1666  halo Uniqlo apakah sedang mencari tim kreatif ...   \n",
       "\n",
       "                                            clean_tweet sentiment  \n",
       "0     tumbler kece by uniqlo new orang silakan reply...   neutral  \n",
       "1              udah gajian tetep sanggup belanja uniqlo  positive  \n",
       "2                   uniqlo dapet ut graphic harga doang   neutral  \n",
       "3              klu besok gw samperin uniqlo deket rumah   neutral  \n",
       "4                               wiiii uniqlo diskon yuk   neutral  \n",
       "...                                                 ...       ...  \n",
       "1662  jaman sma pengen mall kuningan tu pengen liat ...   neutral  \n",
       "1663  sih pengguna uniqlo h amp m levi s sejati hasi...   neutral  \n",
       "1664                                uniqlo diskon lg si   neutral  \n",
       "1665                                             uniqlo   neutral  \n",
       "1666                    halo uniqlo mencari tim kreatif  positive  \n",
       "\n",
       "[1667 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the score result obtained to positive or negative or neutral\n",
    "sentiments = [\"positive\" if sentimen > 0 else \"negative\" if sentimen < 0 else \"neutral\" for sentimen in hasil]\n",
    "df[\"sentiment\"] = sentiments\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23962eef",
   "metadata": {},
   "source": [
    "Next we will see the results of the score obtained on each tweet using the matplotlib module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1ba6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels, counts = np.unique(hasil, return_counts=True)\n",
    "plt.bar(labels, counts, align='center')\n",
    "plt.gca().set_xticks(labels)\n",
    "# Add labels on each bar\n",
    "for i in range(len(labels)):\n",
    "    plt.text(labels[i], counts[i], str(counts[i]), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a468c20f",
   "metadata": {},
   "source": [
    "Before getting into modeling, we have to change the positive, negative, and neutral words to 1, -1, 0. Because machine learning will later have a process of transforming data and it uses numbers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d30d34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GA 2 tumbler kece by Uniqlo new untuk 2 orang ...</td>\n",
       "      <td>tumbler kece by uniqlo new orang silakan reply...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Udah gajian tetep ga sanggup belanja di uniqlo</td>\n",
       "      <td>udah gajian tetep sanggup belanja uniqlo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sering padahal begini Uniqlo pernah dapet UT g...</td>\n",
       "      <td>uniqlo dapet ut graphic harga doang</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...</td>\n",
       "      <td>klu besok gw samperin uniqlo deket rumah</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wiiii uniqlo lagi diskon yuk</td>\n",
       "      <td>wiiii uniqlo diskon yuk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>Jaman SMA pengen ke mall Kuningan apa tu jauh ...</td>\n",
       "      <td>jaman sma pengen mall kuningan tu pengen liat ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>Saya sih pengguna Uniqlo H amp M dan Levi s se...</td>\n",
       "      <td>sih pengguna uniqlo h amp m levi s sejati hasi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>Uniqlo kapan diskon lg si</td>\n",
       "      <td>uniqlo diskon lg si</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>Uniqlo</td>\n",
       "      <td>uniqlo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>1666</td>\n",
       "      <td>halo Uniqlo apakah sedang mencari tim kreatif ...</td>\n",
       "      <td>halo uniqlo mencari tim kreatif</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  GA 2 tumbler kece by Uniqlo new untuk 2 orang ...   \n",
       "1              1     Udah gajian tetep ga sanggup belanja di uniqlo   \n",
       "2              2  Sering padahal begini Uniqlo pernah dapet UT g...   \n",
       "3              3  MASIH ADA GA KLU ADA BESOK GW SAMPERIN UNIQLO ...   \n",
       "4              4                       Wiiii uniqlo lagi diskon yuk   \n",
       "...          ...                                                ...   \n",
       "1662        1662  Jaman SMA pengen ke mall Kuningan apa tu jauh ...   \n",
       "1663        1663  Saya sih pengguna Uniqlo H amp M dan Levi s se...   \n",
       "1664        1664                          Uniqlo kapan diskon lg si   \n",
       "1665        1665                                             Uniqlo   \n",
       "1666        1666  halo Uniqlo apakah sedang mencari tim kreatif ...   \n",
       "\n",
       "                                            clean_tweet  sentiment  \n",
       "0     tumbler kece by uniqlo new orang silakan reply...        NaN  \n",
       "1              udah gajian tetep sanggup belanja uniqlo        NaN  \n",
       "2                   uniqlo dapet ut graphic harga doang        NaN  \n",
       "3              klu besok gw samperin uniqlo deket rumah        NaN  \n",
       "4                               wiiii uniqlo diskon yuk        NaN  \n",
       "...                                                 ...        ...  \n",
       "1662  jaman sma pengen mall kuningan tu pengen liat ...        NaN  \n",
       "1663  sih pengguna uniqlo h amp m levi s sejati hasi...        NaN  \n",
       "1664                                uniqlo diskon lg si        NaN  \n",
       "1665                                             uniqlo        NaN  \n",
       "1666                    halo uniqlo mencari tim kreatif        NaN  \n",
       "\n",
       "[1667 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a value replacement dictionary\n",
    "mapping = {'positif': 1, 'netral': 0, 'negatif': -1}\n",
    "\n",
    "# Replace values in sentiment columns using the replacement dictionary\n",
    "df['sentiment'] = df['sentiment'].map(mapping)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb92426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with positive values: 0\n",
      "Number of records with neutral values: 0\n",
      "Number of records with negative values: 0\n"
     ]
    }
   ],
   "source": [
    "# We will count the number of positive, negative, neutral tweets. \n",
    "# We will compare these results later after using machine learning\n",
    "def count_sentiment():\n",
    "    # Counts the number of records with positive values\n",
    "    count_positif = df[df['sentiment'] == 1]['sentiment'].count()\n",
    "\n",
    "    # Counts the number of records with neutral values\n",
    "    count_netral = df[df['sentiment'] == 0]['sentiment'].count()\n",
    "\n",
    "    # Counts the number of records with negatives values\n",
    "    count_negatif = df[df['sentiment'] == -1]['sentiment'].count()\n",
    "\n",
    "    print(\"Number of records with positive values:\", count_positif)\n",
    "    print(\"Number of records with neutral values:\", count_netral)\n",
    "    print(\"Number of records with negative values:\", count_negatif)\n",
    "count_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45214800",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9ef679c",
   "metadata": {},
   "source": [
    "We have to import some modules first. We will use the Naïve Bayes algorithm. Actually, there are many algorithms that can be used, such as naïve bayes, support vector machine or SVM, a priori, K-means, etc. Why use naïve bayes, because this algorithm only requires a small amount of training data to determine the estimated parameters needed in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "188ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d02e48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As we know to be able to use a machine learning model \n",
    "Then we need data with numeric or integer type, but one of the features we have \n",
    "is text data. Well on this occasion we will utilize the text data change method - matrix \n",
    "what we have learned before, namely TF-IDF\n",
    "'''\n",
    "#Mari we get started, we'll take advantage of the TfidVectorizer method in the sklearn and gaussian Naive Bayes libraries.\n",
    "vectorizer = TfidfVectorizer (max_features=2500)\n",
    "\n",
    "model_g = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b499f31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1667x2500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13100 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lalu we convert our text_clean data into the form of TFIDF Vectorizer\n",
    "v_data = vectorizer.fit_transform(df['clean_tweet']).toarray()\n",
    "\n",
    "vectorizer.fit_transform(df['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d08db6e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e5fbce7013aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#Setelah this let's divide the data into data train and test and then fit it into our model.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(v_data, df['sentiment'], test_size=0.2, random_state=50)\n",
    "model_g.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acad83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dan we do the calculation of confussion matrix, classification report, and accuracy score\n",
    "\n",
    "y_preds = model_g.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_preds))\n",
    "print(classification_report(y_test,y_preds))\n",
    "print('The accuracy score is ',accuracy_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5642641",
   "metadata": {},
   "source": [
    "The Accuracy, Precision, Recall, F1-Score shows that our model is a pretty good model for this case. Support that is too far away also indicates that our dataset sharing is quite bad. If our support is much different, you can try changing the value of random_state so that the division of labels 1, -1, and 0 is more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sekarang let's use it to classify tweet data\n",
    "\n",
    "v_data = vectorizer.transform(df[\"clean_tweet\"]).toarray()\n",
    "y_classification = model_g.predict(v_data)\n",
    "\n",
    "df[\"sentiment\"] = list(y_classification)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of positive, negative, neutral tweets. See the difference\n",
    "count_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ed35d",
   "metadata": {},
   "source": [
    "# Insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jumlah characters in tweets\n",
    "bin_range = np.arange(0, 260, 10)\n",
    "df['clean_tweet'].str.len().hist(bins=bin_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jumlah words in tweets\n",
    "bin_range = np.arange(0, 50)\n",
    "df['clean_tweet'].str.split().map(lambda x: len(x)).hist(bins=bin_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Word length On tweets\n",
    "df['clean_tweet'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dac9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: word_tokenize(str(x)))\n",
    "tweets = [word for tweet in df['clean_tweet'] for word in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b58dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the most outgoing words in a tweet, we use the freqdist function in the ntlk module\n",
    "from nltk.probability import FreqDist\n",
    "fqdist = FreqDist(tweets)\n",
    "\n",
    "most_common_word = fqdist.most_common(20)\n",
    "\n",
    "print(most_common_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then create a visualization for easy viewing\n",
    "fqdist.plot(20,cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biagram analysis\n",
    "result = pd.Series(nltk.ngrams(tweets, 2)).value_counts()[:20]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the last visualization use WordCloud to see what words appear in the Tweet\n",
    "from wordcloud import WordCloud\n",
    "# Combine all reviews for the desired sentiment\n",
    "combined_text = \" \".join([review for review in df['clean_tweet']])\n",
    "\n",
    "# Initialize wordcloud object\n",
    "wc = WordCloud(background_color='white', max_words=50)\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "plt.imshow(wc.generate(combined_text))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
